{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf1c312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Needed for data processing\n",
    "import numpy as np\n",
    "import texthero as hero # Needed for data cleaning\n",
    "from texthero import preprocessing\n",
    "from nltk.corpus import stopwords # Needed for stopwords\n",
    "from HanTa import HanoverTagger as ht # Needed for lemmatization\n",
    "from nltk.tokenize import word_tokenize # Needed for tokenization\n",
    "import pickle # Needed for object export\n",
    "import sys # Needed for system settings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Needed for NLP TF-IDF algorithm\n",
    "from sklearn.metrics.pairwise import cosine_similarity # Needed for cosine similarity\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "np.set_printoptions(threshold = sys.maxsize)\n",
    "\n",
    "hannover = ht.HanoverTagger(\"morphmodel_ger.pgz\") # Needed for German lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a56a75b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reads in dataset\n",
    "df = pd.read_csv(\"../data/raw_data.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Cleans degree_label variable String for certain entries\n",
    "df['degree_label'] = df[\"degree_label\"].str.split('\\r').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3301d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates new columns for id and NLP application\n",
    "df.insert(0, 'major_id', range(0, len(df)))\n",
    "df.insert(3, \"text\", \"NA\")\n",
    "\n",
    "# Concatenates course summary / description of majors with name of the major as well as categories, university and location into the newly created \"text\" column\n",
    "# Many major descriptions only have specific courses listed, but do not contain the major's name. If one only searches for e.g. \"Computer Science\" he might not receive accurate recommendations since usually there is no course or module named like the major itself.\n",
    "# Hence by including the major's name as well as its category and university/location this information is still considered for the recommendation, but downweighted by the number of similar entries in the data, so that it is no automatic guarantee for a top recommendation.\n",
    "df[\"text\"] = df[[\"major_name\", \"category\", \"subcategory\", \"major_category\", \"university\", \"location\", \"major_description\"]].astype(str).agg(\" \".join, axis = 1)\n",
    "\n",
    "# Removes empty description data rows\n",
    "df = df[df[\"major_description\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13581bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defines symbols and other common terms that add no information gain to the algorithm\n",
    "characters = [\"z.B.\", \"(\", \")\", \":\", \".\", \",\", \"|\", \"*\", \"&\", \"+\", \" I \", \" II \", \" III \", \" IV \", \" V \", \" VI \", \" x \", \"\\x96\", \"Semester\", \"ECTS\", \"Bachelorarbeit\", \"Abschlussarbeit\", \"Bachelor\", \"Studium\", \"Grundlagen\", \"Wochen\", \"Auslandssemester\", \"Berufspraktikum\", \"Wahlfach\"]\n",
    "\n",
    "# Cleans text corpus based on previously defined characters\n",
    "for i in range(len(characters)):\n",
    "    df[\"text\"] = [n.replace(characters[i], \"\") for n in df[\"text\"]]\n",
    "df[\"text\"] = [n.replace(\"/\", \" \") for n in df[\"text\"]]\n",
    "df[\"text\"] = [n.replace(\"-\", \" \") for n in df[\"text\"]]\n",
    "df[\"text\"] = df[\"text\"].str.replace('\\d+', '', regex = True)\n",
    "\n",
    "# Further cleans and prepares text corpus\n",
    "custom_pipeline = [preprocessing.fillna,\n",
    "                  preprocessing.lowercase,\n",
    "                  preprocessing.remove_whitespace\n",
    "                  ]\n",
    "df[\"text\"] = hero.clean(df[\"text\"], custom_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca07f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates empty variable\n",
    "token_stop = []\n",
    "\n",
    "# Defines tokenization and lemmatization function\n",
    "def tokenizer_lemmatizer(text, stopwords = token_stop, lemmatize = True, user_input = False):\n",
    "    \n",
    "    # Cleans data if it is user input, else skips this step\n",
    "    if user_input:\n",
    "        for i in range(len(characters)):\n",
    "            text = text.replace(characters[i], \"\")\n",
    "        text = text.replace(\"/\", \" \")\n",
    "        text = text.replace(\"-\", \" \")\n",
    "        \n",
    "    # Lemmatizes data using the hannover.analyze lemmatization model for German language, else only tokenizes\n",
    "    if lemmatize:\n",
    "        tokens = [hannover.analyze(w)[0] for w in word_tokenize(text)]\n",
    "    else:\n",
    "        tokens = [w for w in word_tokenize(text)]\n",
    "    tokens = [w for w in tokens if w not in stopwords]\n",
    "    \n",
    "    return(tokens)\n",
    "\n",
    "# Defines stopwords\n",
    "german_stop_words = stopwords.words(\"german\")\n",
    "# Adapts stop words\n",
    "token_stop = tokenizer_lemmatizer(' '.join(german_stop_words), stopwords = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "950add68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates TfidfVectorizer()\n",
    "vectorizer = TfidfVectorizer(tokenizer = tokenizer_lemmatizer)\n",
    "\n",
    "# Tokenizes and lemmatizes text corpus of the dataset, then creates a sparse matrix of TF-IDF scores\n",
    "tfidf_mat = vectorizer.fit_transform(df[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "851ded99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves df to csv\n",
    "df = df.reset_index().iloc[:,1:]\n",
    "df.to_csv(\"../data/processed_data.csv\", encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9025708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports TF-IDF model as preprocessed pickle objects\n",
    "pickle.dump(vectorizer, open(\"../data/vectorizer.pkl\", \"wb\"))\n",
    "pickle.dump(tfidf_mat, open(\"../data/tfidf_mat.pkl\", \"wb\"))\n",
    "\n",
    "# Exports lemmatized stopwords as pickle object\n",
    "pickle.dump(token_stop, open(\"../data/stopwords.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
